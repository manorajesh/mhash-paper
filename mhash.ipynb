{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# m# - A Cryptographic Hashing Algorithm\n#### Mano Rajesh Robotics Honors\nSpring 2022",
   "metadata": {
    "cell_id": "06660996c9d14c22bf183b3e86207c7b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 156.39999389648438
   }
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "69acb7a7bbb3453585144b4b666819f9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Introduction\nTo test my knowledge of Python, I decided to embark on creating my own hashing algorithm made entirely in Python. A hashing algorithm is where you have a given input and a fixed length output. Simply, this means that the string `hello, world` will have a totally unique output such as `@OOk##x39aVXMyUBu?(*hNx9PGHTjzHCx`. \n<figure><img src=\"/work/mhash-paper/figures/hashing_visual.png\"><figcaption><center><b>Fig. 1 - Visual Example of Hashing, from Brilliant.org; 2022</b></center></figcaption></figure><br>\n\nIn reality, creating a perfect **cryptographic** (a hashing algorithm that's impossible to break) hashing algorithm is practically impossible since the only real way to test all the possibilities is to brute force every piece of data possible and check for collisions. A hashing collision is where _two different pieces_ of data produce the _same or similar_ data. For example, if we go back to our `hello, world` example and `hello, earth` produces that same hash (`@OOk##x39aVXMyUBu?(*hNx9PGHTjzHCx`), then there is a **collision**. In simpler terms, it is when two different input share an output. Collisions means that data can be mimiced, altered, or accessed. If the hashing algorithm is used widely (e.g. SHA, CRC32, etc.) then the collisions can be used to implement malicious code or other malevolent actions.\n\nMy goal was to create a cryptographic hashing algorithm made entirely in Python. This requires quite a bit of math, planning, testing, and analyzing. ",
   "metadata": {
    "cell_id": "8396054e61234f6baebc2fb005505297",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 855.5999755859375
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Development\nTo begin, I started simply using knowledge of encryption algorithms and hashing algorithms to create a base for my own hashing algorithm. This way, I could make my own algorithm rather than making a copy of an existing one. The initial idea for the hashing algorithm was to devire arbitrary and random  \n\n<sub>Note: You can interact with any of the hashing algorithms. Simply type your input, click Apply, and run the code block below</sub>",
   "metadata": {
    "cell_id": "9b2f2ea9837549febc68c242ebbd3412",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 201.60000610351562
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a0d7828d1d98418fb39fe2f17ec118c0",
    "deepnote_variable_name": "plaintext_input_1",
    "deepnote_variable_value": "fasdfasdf",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b6babbd",
    "execution_start": 1653531769809,
    "execution_millis": 13,
    "deepnote_cell_type": "input-text"
   },
   "source": "plaintext_input_1 = 'fasdfasdf'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0742660246b24acd97a285dca9e43e24",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e9258f77",
    "execution_start": 1653531770733,
    "execution_millis": 33,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 399.20001220703125
   },
   "source": "def hashing(plaintext, length=32):\n    seed = 0\n    hash = []\n    salt = 0\n    random_length_num = 1\n    text = \"abcdefghjiklmnopqrstuvwxyz1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ)(*&%$#@!\"\n\n    for char in plaintext:\n        random_length_num += ord(char)\n\n    while salt <= length:\n        seed += ord(plaintext[salt % len(plaintext)]) + salt * random_length_num\n        hash.append(text[(seed**salt*random_length_num) % len(text)])\n        salt += 1\n    return hash\n\nprint(\"\".join(hashing(plaintext_input_1))) ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "jxEbbRYEz7fT2)g%uD*MH5J&J%pXEvsmH\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "##### With a hashing algorithm that _seems_ to work, I began implementing a testing program. Below is the pseudo-code: ",
   "metadata": {
    "cell_id": "70fc37b90a454039a14ad8bc8309cf96",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 54
   }
  },
  {
   "cell_type": "markdown",
   "source": "```\ndef generate string(seed):\n    generate a string like 'a' or 'b' depending on the seed\n\n    if seed is greater than 26\n    return 'aa' or 'ab' depending on seed\n    # incrementing the letter depending on seed\n\nwhile True:\n    hash(generate string(number))\n    write hash result to file\n    increment number by 1\n\nfor word1 in file:\n    for word2 in file:\n        if word2 is similar word2: # Check every word against every other\n            file write word2\n            print(\"Collision found\")\n```",
   "metadata": {
    "cell_id": "ffd70821d1854f5293f27ac82a82407b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 403.6000061035156
   }
  },
  {
   "cell_type": "markdown",
   "source": "This testing program is going to brute force the hashing algorithm. This means that it is very, very slow; however, it is very effective in finding collisions as mentioned previously. Below is a cross-section of one of the outputs. The output (or ciphertext) is on the left, and the input (or plaintext) is on the right. This format allows for very easy reading, analyzing, and error-checking because it is very simple and consistent.",
   "metadata": {
    "cell_id": "0041ab6425534801a9a956d36c0769b1",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 119.60000610351562
   }
  },
  {
   "cell_type": "markdown",
   "source": "```\n.\n.\n.\nqx@6JYGBY01SnvviOs2Mz1IzlJ?njcCmq jjjj\n7v!m53p@z)YhLOXJx&mB19&UU5?4ER84P kkkk\n8t<%p@Z3(qv8jgyj@D!12Gno3p?J?63Kn llll\nLr?LaB9Y3HTXG0)IGnTp3O9Jaa?Z6ix(M mmmm\nEpa3Wfht%!qmd*2hpYEe4WTdJW?e(Yshk nnnn\nwnbiHKRP6yOCBu&HZ9z%5%d9rH?uwCnyJ oooo\n8lc(3o1k@Pl*<N5g9jkT6by&)3?ASqjFh pppp\noidInT?G9fJr7f#GhT#I7iJx9n?Qn$dWG qqqq\nvhez<xJb?7gH%98fR4R88r%Sg<?#JJ<ce rrrr\nXffgU*s8BXE@2(<F1dCw9zomPU?lex%tD ssss\ncdgYF7*&cnbwZtAe?Oxl080HxF?2AbZAb tttt\ndbhF1aByEE0MwMbEJyjaAFUb#1?H#QURA uuuu\nr?jwlFkUf$@bUeDds<%)BNe7El?X25P!< vvvv\nk!id@iUpHv52r8eD*JPPCVz(m@?cXjKo8 wwww\n&#kVSO4LjM*RP)GcBtAED&KvVS?ssXF6# xxxx\nd%lCDscgKczgmshCk%v4Ea$Q4D?9OBAM5 yyyy\nV*mtyXMCl4X7KLJbUEgsFjpkby?Oip6&& zzzz\n$qmkF%XC%?)wL9v0VziexJ1csv?4G1bmw aaaa\nfon*197!61xMj(X<5?$%yRL8(g?Jbe@4V bbbbb\nXmoJlcf4@RVbGty9dKQTzZ#*0*?Z8T*Kt ccccc\n%kp1@HPZ9hs2dM)!NuBI1@qwhN?e&8X(S ddddd\n6jqhSlyu?9QRBe28w$w82eBRQ9?uylShq eeeee\nLgrZDQ!QBZng<8&@#Fhw3mWlyt?AU)NyP fffff\nMesGyuHlcpL77)57Fp&l4ugG@e?QpEIFn ggggg\n)ctxiZqHEGjW%s##o)Oa532aF)?#LsDWM hhhhh\nTaue$4)cf@Gl2L86YA0)6AM6nL?lg@9ck iiiii\nB<vWQ!09HxdBZd<$8kuP7I@)W7?2CL4tJ jjjjj\nM@wDBCj%jOB(w7A5gVfE8Qru5r?H!zyAh kkkkk\n4$xuwgSzKe<qUZb%Q6(49YCPcc?X4dtRG lllll\nA&ybhL2Vl67GrrD4zfMs0#XiLY?cZSo!e mmmmm\nb(zT&paqNW%#PKe&<Q8hAdhEtJ?su7ioD nnnnn\nrZ1AOUKMom2vmcG3I1s@Bl3<*5?9Qke6b ooooo\n.\n.\n.\n```",
   "metadata": {
    "cell_id": "a6bd9d3b62844632ac47e1ed01324c64",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 840.4000244140625
   }
  },
  {
   "cell_type": "markdown",
   "source": "The amount of collisions found in the first version of the hash was signifcant; there were around a 1000 collisions in a million hashing. In other words, there was one collision for every 1000 hashes. This may seem like a practical amount of collisions (only 0.1% of hashes are collisions); however, SHA or the Secure Hashing Algorithm (the standard hashing algorithm for the US Government) has **no** collisions at all. \n\nIn light of this, I aimed to deconstruct my hashing algorithm and find how those collisions were created.",
   "metadata": {
    "cell_id": "126ee46f21f34c06a754998fc9a03ee4",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 156
   }
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "258d37d033614b0f8419db39afe6b14c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "To understand the hashing algorithm, we must deconstruct each section of the algorithm. Broadly, the algorithm is split into two parts: a number linked to the length of the input and the assignment of letters to output. Firstly, the number linked to the length of the input is important since you want the length of the input to drastically change the output. If it is drastically different, the input `i` and `ii` should be very different even though they contain the same character.\n\nWith that said, this version of the hash does not account for this as seen by the example of `gn` and `gnof`. the hash for `gn` is `bbsdO54K4B7%J7V7Bwiu7fb0JDs9Sma!t`, yet the hash for `gnof` is `bbddO5AK4BB%J7S7Bwmu7fq0JDp9SmM!t`. The resultant hashes are extremely close since the input strings are also very similar to each other. This is a major fault since an attacker can use this knowledge to estimate the data in a hash. For example, if the attacker knows that a particular secret hash is similar to another hash, they can estimate what the secret data is by using the hash input they already know.",
   "metadata": {
    "cell_id": "be6bb70452f642ba8597e2186425692d",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 245.60000610351562
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5b24f1194f43488389a593ed2827e578",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "e309c0a6",
    "execution_start": 1653507686450,
    "execution_millis": 5,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 309.20001220703125
   },
   "source": "plaintext = plaintext_input_1\n\nseed = 0\nhash = []\nsalt = 0\nrandom_length_num = 1\ntext = \"abcdefghjiklmnopqrstuvwxyz1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ)(*&%$#@!\"\n\nfor char in plaintext:\n    random_length_num += ord(char)\n\nprint(random_length_num)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "427\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6faff10f842e48dd9027bd56bded2060",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7acfe4e0",
    "execution_start": 1653507688278,
    "execution_millis": 15,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 201.1999969482422
   },
   "source": "while salt <= 32:\n    seed += ord(plaintext[salt % len(plaintext)]) + salt * random_length_num\n    hash.append(text[(seed**salt*random_length_num) % len(text)])\n    salt += 1\n\nprint(\"\".join(hash))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "bbddO5AK4BB%J7S7Bwmu7fq0JDp9SmM!t\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "This section of the hash is where in combination with the `seed` variable, `salt` variable, and `random_length_num` variable come together to calculate a random (yet consistent) letter for each place in the hash. Since the length of the hash is 32 characters, the loop will go on 32 times.\n\nSince the `random_length_num` variable is vulnerable, the entire main loop is thereby affected. This area is also of concern since it has a lot of very compute-intensive calculations (like exponents and division). This means that this area is an inefficient part of the algorithm and can be improved.",
   "metadata": {
    "cell_id": "51247ba119da4701b1614fdfb273bc98",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 178.39999389648438
   }
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "d40389c717f44b2486d095437f6c6c03",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "Now, the algorithm can be improved if the varible `random_length_num` is changed drastically with the input length, and the character assignment section is adjusted for speed. Below is the new algorithm:",
   "metadata": {
    "cell_id": "9bf9f39f81dc48a980e5ad23bbd89818",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.80000305175781
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5a53366986664816a24a2e6f40c3e332",
    "deepnote_variable_name": "plaintext_input_1",
    "deepnote_variable_value": "this is a neuasjdf;lkasjdfkl",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e584150",
    "execution_start": 1653531606396,
    "execution_millis": 12,
    "deepnote_cell_type": "input-text"
   },
   "source": "plaintext_input_1 = 'this is a neuasjdf;lkasjdfkl'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a052cd3521e94b8eb873ae390de7d81a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d8e0c8cd",
    "execution_start": 1653531607446,
    "execution_millis": 28,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 471.20001220703125
   },
   "source": "plaintext = plaintext_input_1\n\ndef hashing(plaintext, length=32):\n    seed = 0\n    hash = []\n    salt = 0\n    random_length_num = 1\n    text = \"abcdefghjiklmnopqrstuvwxyz1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ)(*&%$#@!\"\n\n    for char in plaintext:\n        random_length_num += ord(char)\n        random_length_num += ~ len(plaintext)\n\n    while salt <= length:\n        seed += ord(plaintext[salt % len(plaintext)]) + salt * random_length_num\n        hash.append(text[(seed**salt*random_length_num) % len(text)])\n        salt += 1\n        random_length_num += 1\n    return hash\n\nprint(\"\".join(hashing(plaintext_input_1))) ",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "R3Fb5LFixpGp&kP*rkaoapsBmla#kt2SQ\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "The algorithm was changed very minimally, but those changes were very important. The first and most important change was the line `random_length_num += ~ len(plaintext)`. This was important as the result of the line is heavily dependent on the length of the input. This is beacuse it takes the length of the input and inverts the bits (0 will be 1 and 1 will be 0). This is then added to the `random_length_num` variable to get a large number. The variable is then dependent on not only the actual character inputted (as shown with the `ord()` function) but also the length of the input.\n\nAdditionally, I added the line `random_length_num += 1` within the second part in order to make the chance of repeated values of it (with different inputs) nearly impossible.\n\nThese two changes made the chance of collisions nearly impossible with no collisions found in 10,000 hashes and only **one** collision found in 1,000,000 hashes.",
   "metadata": {
    "cell_id": "fda278c2fc89490b9d5ca0474bb3a56c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 259.6000061035156
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Analysis",
   "metadata": {
    "cell_id": "06ec5ce8fdec4e7c948722885efba279",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 82
   }
  },
  {
   "cell_type": "markdown",
   "source": "To analyze the result, I used the aforementioned testing algorithm, hashing time program, and a plotting program.\n\nBeginning with the testing algorithm, the algorithm was designed around the idea that a lot of hashes would be generated, and each hash would be compared against each other to find collisions. With this, there would have to be 2 sections to the tester: word and hash generator and the comparison between the hashes.",
   "metadata": {
    "cell_id": "46fba5bec87347878023b737908297d3",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 133.60000610351562
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0afb7f11c3194f199a4ac9bd1fd7de3e",
    "deepnote_variable_name": "seed_input_1",
    "deepnote_variable_value": "683",
    "deepnote_slider_min_value": 0,
    "deepnote_slider_max_value": 10000,
    "deepnote_slider_step": 1,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f36a27f1",
    "execution_start": 1653531795175,
    "execution_millis": 2244,
    "deepnote_cell_type": "input-slider"
   },
   "source": "seed_input_1 = 683",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8e3848b75ddd4bca94a3c0776edf8d4e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6fbaf1c4",
    "execution_start": 1653531797334,
    "execution_millis": 589,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 454.3999938964844
   },
   "source": "seed = seed_input_1\n\ndef lowercase_word(seed):\n    word = \"\"\n    seed *= 1\n    while seed > 0:\n        word += chr(seed % 26 + 97)\n        seed = seed // 26\n    return word\n\ndef multiple_letters(seed):\n    word = \"\"\n    while seed > 0:\n        word += chr(seed % 26 + 97)\n        seed -= 26\n    return word\n\nprint(lowercase_word(seed))\nprint(multiple_letters(seed))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "hab\nhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "The generation of the inputs is quite easy: depending on the `seed` variable, return a series of letters. This sequential generation of inputs is quite efficient. This allows this step to take very little time and compute so that the analysis can use those resources.",
   "metadata": {
    "cell_id": "fd14a200f9974094b95777363b19c1e9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.80000305175781
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0fc47ed040984a279370827904b0a1e6",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 354
   },
   "source": "def thread_function(hashes):\n    file2 = open(\"hash_collisions.txt\", \"a\")\n    try:\n        counter = 0\n        for line in hashes:\n            for word in hashes:\n                if jf.jaro_distance(line.split()[0], word.split()[0]) > 0.9 and line != word:\n                    file2.write(line + word + '\\n')\n\n            if counter % 500 == 0:\n                print(counter, time.asctime())\n            counter += 1\n        file2.close()\n    except KeyboardInterrupt:\n        print(\"Keyboard interrupt\")\n        file2.close()\n        exit()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Within this function, it first opens the file to write to if there are collisions found. Then, the two `for` loops are what loops through all the elements in the `hashes` list. As seen in Fig. 2, the first loop is the hash on the left that is then checked with every other hash found with the second loop. This is a simple, linear way to compare the hash to everything else.\n\n<figure><img src=\"/work/mhash-paper/figures/hashes.png\"><figcaption><center><b>Fig. 2 - Visual Example of Collision Checking, from Mano Rajesh; May 24, 2022</b></center></figcaption></figure><br>\n\nThis method, although very comprehensive, is very slow since the function has to repeat this operation $number\\;of\\;hashes^2$ times since each hash needs to be compared to $n$ hashes. This is a very inefficient and is a prime candidate for multi-processing. ",
   "metadata": {
    "cell_id": "914e1583b883404f9a87ebf4482f0c91",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 451.1166687011719
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c62362aa0c02427a97039692fe7b9788",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 264
   },
   "source": "# mp.cpu_count() returns the number of usable hardware CPU cores\n# text_buffer is the contents of the file with the hashes\n\nfor i in range(mp.cpu_count()):\n    end = start + len(text_buffer) // mp.cpu_count()\n    p = Process(target=thread_function, args=(text_buffer[start:end],))\n    p.start()\n    start = end\n\nfor i in range(mp.cpu_count()):\n    p.join()\nprint(\"Done at \" + time.asctime())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "This section simply divides the hash file into equal (or nearly) pieces to be sent independently to the CPU cores. This division of the file is shown with the `end` variable assignment line: `end = start + len(text_buffer) // mp.cpu_count()`. This would create a cross-section of the data that is then sent to the core. After, this assignment is offset by the previous cross-section so that each core gets an equal amount of work (`start = end`).\n\nWith multi-processing, instead of one CPU core computing $10000^2$ or $100000000$ collision checks sequentially for example, 4 CPU cores can compute $({\\frac {10000}{4}})^2=2500^2$ or $6250000$ collision checks simultaneously. There is a difference of 93,750,000 checks for one CPU core.\n\nThe speed improvements are quite significant: With 1000 hashes, single-core execution time is 31.3 seconds while four-core execution time is 1.2 seconds. In other words, the multi-core execution takes 3.8% of the original time.\n\n--",
   "metadata": {
    "cell_id": "fd71aec894884e49866d200db0ac974e",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 296.1499938964844
   }
  },
  {
   "cell_type": "markdown",
   "source": "With any collisions found with the testing, I would then input them into the below program to find the similarity of the two hashes with the Jaro–Winkler Distance equation (also used by the testing program).",
   "metadata": {
    "cell_id": "197aae871b674025b80880704a43fbc5",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 74.80000305175781
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4440bd812950479e8b37f773776fe565",
    "deepnote_variable_name": "plaintext_input_2",
    "deepnote_variable_value": "fg",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "16e12161",
    "execution_start": 1653533533311,
    "execution_millis": 206,
    "deepnote_cell_type": "input-text"
   },
   "source": "plaintext_input_2 = 'fg'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c141cb42f870466aaa299aa5b2362eb0",
    "deepnote_variable_name": "plaintext_input_3",
    "deepnote_variable_value": "f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "72fdbd26",
    "execution_start": 1653533499874,
    "execution_millis": 1,
    "deepnote_cell_type": "input-text"
   },
   "source": "plaintext_input_3 = 'f'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d7d44361398e434d8cf61dc9e764f7d6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "96e8e097",
    "execution_start": 1653533534595,
    "execution_millis": 38,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 599.5999755859375
   },
   "source": "import jellyfish as jf\n\ndef hashing(plaintext, length=32):\n    seed = 0\n    hash = []\n    salt = 0\n    random_length_num = 1\n    text = \"abcdefghjiklmnopqrstuvwxyz1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ)(*&%$#@!\"\n\n    for char in plaintext:\n        random_length_num += ord(char)\n        random_length_num += ~ len(plaintext)\n\n    while salt <= length:\n        seed += ord(plaintext[salt % len(plaintext)]) + salt * random_length_num\n        hash.append(text[(seed**salt*random_length_num) % len(text)])\n        salt += 1\n        random_length_num += 1\n    return hash \n\nplaintext1 = plaintext_input_2\nplaintext2 = plaintext_input_3\n\nprint(\"\".join(hashing(plaintext1)))\nprint(\"\".join(hashing(plaintext2)))\nprint(jf.jaro_distance(\"\".join(hashing(plaintext1)), \"\".join(hashing(plaintext2))))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "W2sM1%fjQIKExab6mm7PlngkG2x143Zb5\n5HNe&WmZCVzGQ#09)ty7e#8aOSF$E7E$8\n0.3434343434343434\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "61f41d59316a4b4eb2839672a6b6c1a9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "Now with a practical hashing function, the step was finding and quantifying the amount of time needed to compute the hashes. In order words, how compute-intensive is the hashing function. Additionally, finding the Big O number was important as it describes how intensity increases (e.g. $O(n^2)$ is exponentially increasing in intensity).\n\nTo find this, a simple hasher and timer program was created.",
   "metadata": {
    "cell_id": "47b1e89cabe34dd08008193d516ec9b8",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 133.60000610351562
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c15f5fe6608e44a5b2407b9672eeec41",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 408
   },
   "source": "arguments = []\nfile = open(\"hash-times.txt\", \"w\")\nfile.close()\n\nfor i in range(1000):\n    arguments.append(\"xg\"*(i+1))\n\nfor i in range(1000//mp.cpu_count()):\n    try:\n        processes = []\n        for j in range(mp.cpu_count()):\n            processes.append(mp.Process(target=hashing, args=(arguments[i*mp.cpu_count()+j],)))\n        for process in processes:\n            process.start()\n        for process in processes:\n            process.join()\n    except (KeyboardInterrupt or IndexError):\n        for process in processes:\n            process.terminate()\n        break",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Firstly, the first `for` loop makes a list with the string `xg` in an increasing order (i.e. `arguments = ['xg', 'xgxg', 'xgxgxg'...]`). I chose this method because this mean if you time the hashing algorithm hashing each element of the `arguments` list, you will get an increasing time that illustrates how input length affects time.\n\nThe second `for` loop is the core assignment of the hashing and timing function. This has a very similar structure to the collision tester. The main difference is that the process (i.e. the different functions being sent to the individual cores) is put into a list and started from there. This was used because the `arguments` variable is split into pieces, and then those pieces are sent to the CPU cores. This was chosen since the hashing algorithm takes one element on the list as an argument, not a list (slices can't be used). This method means that several hashes can be produced simultaneously, decreasing the amount time to compute execution times.\n\n--",
   "metadata": {
    "cell_id": "638c69ee9fe5438dbf7f2f50115e44e6",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 259.6000061035156
   }
  },
  {
   "cell_type": "markdown",
   "source": "Now, with a list of the execution times, the data can be plotted to visualize and compute the equation to find the complexity of the hashing algorithm. The plotter is split into 5 distinct parts: library imports and function definitions, data preparation, line of best fit calculations, difference graphs, and plotting. ",
   "metadata": {
    "cell_id": "511e3d24faef49d0b08a342e3a81819b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 97.19999694824219
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d9a9e316d4f4442a8fe256b4853cab6d",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 534
   },
   "source": "from statistics import fmean\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage.filters import gaussian_filter1d\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef horizontal_line(limit, y):\n    return [y for i in range(limit)]\n\n# Exponential formula\ndef exponential(x, a, b):\n    return a*np.exp(b*x)\n\n# My usual line of best fit for exponential\ndef line_of_best_fit(x1, x2, y1, y2):\n    b = (y2/y1)**(1/(x2-x1))\n    a = y1\n    print(f\"y = {a}*{b}^x\")\n    \n    values = [a*b**i for i in range(x2)]\n    return values, f\"y = {a}({b})^x\"\n\ndef average_graphs(y_val1, y_val2):\n    average_graph = []\n    for i in range (len(y_val1)):\n        average_graph.append(fmean([y_val1[i], y_val2[i]]))\n    return average_graph",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Firstly, the libraries `statistics, matplotlib, scipy, and numpy` were used. Their use is explained below. The `horizontal_line()` function is used for clarity on the final graph. The `exponential()` function is used as a model equation for the `curve_fit()` function used later on. The `line_of_best_fit()` function is used to calculate the *exponential* line of best fit between two points. Lastly, the `average_graphs()` function takes two set y-values of two graphs and averages them.",
   "metadata": {
    "cell_id": "3fbd2a453802449db7dfef5534420eb9",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 119.60000610351562
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "254be3e662a44ed8a4dacd1b10676305",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 210
   },
   "source": "########### Data Preparation ###########\n\n# File reading\nfile = open(\"tests\\hash-times_50000.txt\", \"r\")\ntimes = file.read().split()\ntimes = [float(i) for i in times]\n\n# Smooth the data with a gaussian filter\ntimes_smoothed = gaussian_filter1d(times, sigma=25)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The raw data is then read from the `hash-times.txt` file produced by the function previously described. Each entry in that file is the time in fractional seconds seperated by newlines. The data read from the file is a string, so in order to use them for calculations, each element is converted into a float with the line `times = [float(i) for i in times]`. After the raw data is prepared, that data is then smoothed with a 2D Gaussian Filter since the data is quite irregular. Pictured below is the **1D Gaussian Filter** where $x$ is the data to be smoothed to the degree of $\\sigma$. In simpler terms, $x$ is smoothed by $\\sigma$.\n\n$$\nG(x) = \\frac{1}{\\sqrt[]{2 \\pi } \\sigma} e^{- \\frac{x^{2}}{2 \\sigma ^{2}}}\n$$",
   "metadata": {
    "cell_id": "f41054fadf5f4e70bfbfadb66ebe5906",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 208.13333129882812
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1c37e1270c054a53b69c4452216b5914",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 246
   },
   "source": "########### Line of Best Fit ###########\n\n# Find line of best fit with usual formula\nlineofbestfit1, equation = line_of_best_fit(0, len(times), times_smoothed[0], times_smoothed[-1])\n\n# Find line of best fit with curve_fit\npars, cov = curve_fit(f=exponential, xdata=range(len(times_smoothed)), ydata=times_smoothed, p0=[0, 0], bounds=(-np.inf, np.inf))\nlineofbestfit2 = exponential(range(len(times_smoothed)), *pars)\n\n# Find line of best fit by averaging the two lobfs\naverage_lobf = average_graphs(lineofbestfit1, lineofbestfit2)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "The smoothed data is then used to calculate the lines of best fit with several different methods. For `lineofbestfit1`, it is found with the equation:\n$$\n({\\frac{y_2}{y_1}})^{\\frac{1}{x_2-x_1}} \n$$\nThis has the benefit of being simple; however, it only used two seperate points on the graph, so it is not the most accurate.\n\n--\n\n`lineofbestfit2` was found using the library function `scipy.curve_fit()`. This function takes $data$ and models (or _fits_) it to a given curve or equation. This turned out to be the most accurate line of best fit but only for certain datasets.\n\n--\n\n`average_lobf` was found by averaging both `lineofbestfit1` and `lineofbestfit2` in order to get a slightly different ouput. This was supposed to aid with that fact that the first line of best fit was most accurate at the ends of the function but not the center like the second line of best fit.\n$$\ny_\\mu={\\frac{y_2-y_1}{2}} \\quad\\quad x_\\mu={\\frac{x_2-x_1}{2}}\n$$",
   "metadata": {
    "cell_id": "78cca593d23d441bb8798a916819b1d7",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 432.6499938964844
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8523b2377a894cc6b493aa885f252936",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 318
   },
   "source": "########### Difference Graphs ###########\n\n# Find difference between parent graph and first line of best fit\ndifference_graph1 = []\nfor i in range(len(times_smoothed)):\n    difference_graph1.append(times_smoothed[i] - lineofbestfit1[i])\n\n# Find difference between parent graph and second line of best fit\ndifference_graph2 = []\nfor i in range(len(times_smoothed)):\n    difference_graph2.append(times_smoothed[i] - lineofbestfit2[i])\n\ndifference_graph3 = []\nfor i in range(len(times_smoothed)):\n    difference_graph3.append(times_smoothed[i] - average_lobf[i])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "To find the accuracy of the lines of best fit (i.e. LOBF), the difference between each output of the graph can be plotted. In other words, the closer to 0 the difference graph is, the more accurate it is. To do this, a simple `for` loop iterates through each output (or y-value) of the LOBFs and subtracts them. The difference is saved to a list where it is then graphed.",
   "metadata": {
    "cell_id": "ed89762a8d7d4615bce59b4e2520f9c7",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 97.19999694824219
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e1fe47fbc0854564a8068b2bb1b278b1",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 660
   },
   "source": "########### Plotting Graphs ###########\n\nprint(f\"differnce_graph1: {fmean(difference_graph1)} \\ndifference_graph2: {fmean(difference_graph2)} \\ndifference_graph3: {fmean(difference_graph3)}\")\nprint(min((fmean(difference_graph1)), fmean(difference_graph2), fmean(difference_graph3)))\n\n# Plot the data graphs\nfig, ax = plt.subplots(2)\nax[0].plot(times_smoothed, color=\"black\")\nax[0].plot(lineofbestfit1, linestyle='dashdot', color=\"green\")\nax[0].plot(lineofbestfit2, linestyle=':', linewidth=2, color='blue')\n#ax[0].plot(average_lobf, color=\"red\")\n#ax[0].plot(lineofbestfit3, color=\"orange\")\n\n# Plot the difference graphs\nax[1].plot(difference_graph1, color=\"green\")\nax[1].plot(difference_graph2, color=\"blue\")\n#ax[1].plot(difference_graph3, color=\"red\")\nax[1].plot(horizontal_line(len(times_smoothed), 0), linestyle=':')\n\n# Formatting\nax[0].set_title(file.name, y=1.2, pad=-14)\nax[0].set_ylabel(\"Time (s)\", labelpad=10)\nax[0].set_xlabel(\"Hash Length\", labelpad=10)\nax[1].set_ylabel(\"Difference (s)\", labelpad=10)\nax[1].set_xlabel(\"Hash Length\", labelpad=10)\nax[0].legend([\"Data\", \"Line of Best Fit\", \"Line of Best Fit (curve_fit)\"], loc=\"upper left\")\nax[1].legend([\"Difference (Line of Best Fit)\", \"Difference (Line of Best Fit (curve_fit))\", \"Reference Horizontal Line\"], loc=\"upper left\")\n\n\nax = plt.gca()\nax.set_yticks(ax.get_yticks()[::1]) # Set y-ticks to every second value\nplt.grid(False) # Remove grid\nfile.close()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "To plot the graphs, the library `matplotlib` was used to easily plot a list of values to a GUI. The smoothed data and LOBFs are plotted on the first subplot and color-coded. The difference graphs are then plotted on another subplot and color-coded to match its parent graph. From there, the plots' ticks are set to appear every other tick for clarity. Finally, labels and a legend are added to the axses for readability. All these formatting modules are created with `matplotlib`.\n\n--",
   "metadata": {
    "cell_id": "ed1dfa6928e64dd6a58e524dbb0c3665",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 156
   }
  },
  {
   "cell_type": "markdown",
   "source": "Several hash-time datasets were tried: 1000 hashes, 10000 hashes, 20000 hashes, and 50000 hashes. Among those, a few were single-threaded to check for stability in the multi-processing tester. Below are the figures found with this plotter:\n\n<figure><img src=\"/work/mhash-paper/figures/hash_times_10000.png\"><figcaption><center><b>Fig. 3 - Graph of the hashing times, from Mano Rajesh; May 24, 2022</b></center></figcaption></figure><br>\n<hr>\n<figure><img src=\"/work/mhash-paper/figures/hash_times_20000.png\"><figcaption><center><b>Fig. 4 - Graph of the hashing times, from Mano Rajesh; May 24, 2022</b></center></figcaption></figure><br>\n<hr>\n<figure><img src=\"/work/mhash-paper/figures/hash_times_50000.png\"><figcaption><center><b>Fig. 5 - Graph of the hashing times, from Mano Rajesh; May 24, 2022</b></center></figcaption></figure><br>\n",
   "metadata": {
    "cell_id": "8da15953669b48b1a47126c045e8b9ab",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 2929.199951171875
   }
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "92dbae885211440dade5e3ef3e996f5c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "Finally, to find the **Big-O number**, I guessed and checked matching different curves to the original dataset. Using *Desmos's Graphing Calculator*, I inputting values to match a several different curve equations (e.g. quadratic, rational, exponential, etc.) to the original dataset. Eventually, the quadratic curve fit the dataset the best, so the Big-O number would be quadratic. In other words, the Big-O for this hashing algorithm is $O(n^2)$. This means that the algorithm increases exponentially (to the second degree) in compute-intensity with input length.\n\nThe exact function for the Big-O estimate is,\n$f(x) = x^{2}\\cdot0.00000000023+0.0166$\n\n<figure><img src=\"/work/mhash-paper/figures/big-o-estimate.png\"><figcaption><center><b>Fig. 6 - Graph of the hashing time and Big-O estimate, from Mano Rajesh; May 24, 2022</b></center></figcaption></figure><br>",
   "metadata": {
    "cell_id": "4075cac7cd54450799e84aaafc9c8b1b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 724.816650390625
   }
  },
  {
   "cell_type": "markdown",
   "source": "---",
   "metadata": {
    "cell_id": "447320f352654470bceb70b079da1a1b",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 46
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Potential Improvements\nEven though the m# (Mano Hash) hashing algorithm is pretty good at the moment, there are a few improvements to be implemented.\n\n<hr>\n\n|                               \t|                                                                                                                                                                                                                                                                                                                                                   \t|\n|-------------------------------\t|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n|     **Bitwise Operations**    \t| Rather than picking random numbers and cycling through a set of letters (as represented by the `text` variable in the function), operating directly on the bits of the input mean that regardless of the input data, the hash will be unique. Additionally, there is a potential that bitwise operations would be faster than integer operations. \t|\n| <br>                          \t|                                                                                                                                                                                                                                                                                                                                                   \t|\n|    **Faster Time to Hash**    \t| In correlation with bitwise operations, making the second `for` loop faster by replacing pointless, compute-intensive operations for clean, quick bitwise operations.                                                                                                                                                                             \t|\n| <br>                          \t|                                                                                                                                                                                                                                                                                                                                                   \t|\n| **Refined <br>Testing Algorithm** \t| Since many of the hash collisions share input characters, I can only test a hash against other hashes with similar input characters. This would reduce the testing time by magnitudes, and the effectiveness of the tester should be the same or similar.                                                                                         \t|",
   "metadata": {
    "cell_id": "d79d981f5a8544dea0cd0d4c4a56253c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 373.79998779296875
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Conclusion\nThe **m# hashing algorithm** is not yet cryptographic like the title implies: that requires heavy cryptanalysis. With that said, this hashing algorithm is quite good with encoded strings as shown throughout this paper. It also functions quite well with large files and large inputs. As illustrated throughout this paper, the hashing alorithm was tested against encoded strings (implying numbers but not explicitly tested), and it shines in its simple calculations. With that said, the algorithm is still in its infancy and cannot compete with more mature algorithms like SHA or xxHash. \n\nThis was a project that tested my skills with a language I am only beginning to understand. This project touched nearly all aspects of the language, testing me to find the most efficient approach to a problem.\n\nYou can visit the GitHub repository [here](github.com/manorajesh/hashing) for an in-depth view of the project.",
   "metadata": {
    "cell_id": "3b7a46e51bac48cb9d62a7f1be301fce",
    "tags": [],
    "owner_user_id": "0ff3066a-5020-4d17-945f-b23a349ca8b1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 555.2000122070312
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=764b8806-990e-4d16-80ea-8c890893c289' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "6ebb1a1d-149a-430e-9cbd-797aea61e977",
  "deepnote_execution_queue": []
 }
}